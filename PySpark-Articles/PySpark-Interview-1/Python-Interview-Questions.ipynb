{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba940fd9-c6fe-4de0-a5aa-bf2c48dd999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_PYTHON_DRIVER\"] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cce9ae8-4989-4dbc-a9e6-39b7f37e838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Article\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e418502f-38b5-454d-bd6a-eff5e61b9a77",
   "metadata": {},
   "source": [
    "### Question 1 : Write a pyspark code to generate the below output for the given input dataset(Asked in service based companies)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ebe45ca-ed0e-4186-8a7c-2138084f7490",
   "metadata": {},
   "source": [
    "+------+---------+----------+-----------+---------+\n",
    "|EmpId |Name     | Locations                      |\n",
    "+------+---------+----------+-----------+---------+\n",
    "|1     |Gaurav   | Pune     , Bangalore, Hyderabad|\n",
    "|2     |Risabh   | Mumbai   ,Bangalore, Pune      |\n",
    "+------+---------+----------+-----------+---------+\n",
    " \n",
    "Required Output\n",
    "EmpId Name Location\n",
    "1     Gaurav  Pune\n",
    "1     Gaurav  Bangalore\n",
    "1     Gaurav  Hyderabad\n",
    "2     Risabh  Mumbai\n",
    "2     Risabh  Pune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f6a8eff-b37c-4397-bfe7-77dcc4429f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, \"Gaurav\", \"Pune, Bangalore, Hyderabad\"),\n",
    "       (2, \"Rishabh\", \"Mumbai, Bangalore, Pune\")]\n",
    "cols = [\"EmpId\", \"Name\", \"Locations\"]\n",
    "df = spark.createDataFrame(data, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bf5e4e6-131c-4ff1-8ff5-bb2f3fa50ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----------+\n",
      "|EmpId|   Name| Locations|\n",
      "+-----+-------+----------+\n",
      "|    1| Gaurav|      Pune|\n",
      "|    1| Gaurav| Bangalore|\n",
      "|    1| Gaurav| Hyderabad|\n",
      "|    2|Rishabh|    Mumbai|\n",
      "|    2|Rishabh| Bangalore|\n",
      "|    2|Rishabh|      Pune|\n",
      "+-----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split, explode\n",
    "df.select(df.EmpId, df.Name, explode(split(df.Locations,\",\")).alias(\"Locations\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780ec646-2e69-4ceb-a8fa-7858518a81ac",
   "metadata": {},
   "source": [
    "### Question 2.Spark on Windows â€” What exactly is winutils and why do we need it? Also How ir relates the PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbbe98b-4ab8-4489-aa4e-59f77d286b6f",
   "metadata": {},
   "source": [
    "- Hadoop requires native libraries on Windows to work properly -that includes accessing the file:// filesystem, where Hadoop uses some Windows APIs to implement posix-like file access permissions.\n",
    "- This is implemented in HADOOP.DLL and WINUTILS.EXE. *\r\n",
    "In particular, %HADOOP_HOME%\\BIN\\WINUTILS.EXE must be locatab.\n",
    "- I know of at least one usage, it is for running shell commands on Windows OS. You can find it in org.apache.hadoop.util.Shell, other modules depends on this class and uses it's methods.\n",
    "- PySpark is an interface for Apache Spark in Python.\n",
    "- It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment.\n",
    "- PySpark supports most of Spark's features such as Spark SQL, DataFrame, Streaming, MLlib (Machine Learning) and Spark Core.le"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699d0ca6-53c3-4ac9-9656-f2eeb55662bb",
   "metadata": {},
   "source": [
    "### Question 3.What's the Difference Between Hadoop and Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e21806-3965-4318-8536-98c493f54103",
   "metadata": {},
   "source": [
    "- Apache Hadoop and Apache Spark are two open-source frameworks you can use to manage and process large volumes of data for analytics.\n",
    "- Organizations must process data at scale and speed to gain real-time insights for business intelligence.\n",
    "- Apache Hadoop allows you to cluster multiple computers to analyze massive datasets in parallel more quickly.\n",
    "- Apache Spark uses in-memory caching and optimized query execution for fast analytic queries against data of any size.\n",
    "- Spark is a more advanced technology than Hadoop, as Spark uses artificial intelligence and machine learning (AI/ML) in data processing.\n",
    "- However, many companies use Spark and Hadoop together to meet their data analytics goals.lity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e90ceb-3528-4e8b-9ba3-062a9c496538",
   "metadata": {},
   "source": [
    "Distributed big data processing :\n",
    "- Big data is collected frequently, continuously, and at scale in various formats\n",
    "- To store, manage, and process big data, Apache Hadoop separates datasets into smaller subsets or partitions.\n",
    "-  It then stores the partitions over a distributed network of servers.\n",
    "-  Likewise, Apache Spark processes and analyzes big data over distributed nodes to provide business insights.\n",
    "- Depending on the use cases, you might need to integrate both Hadoop and Spark with different software for optimum functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f36cde2-b9f6-4f6b-9c6e-186f3611f475",
   "metadata": {},
   "source": [
    "Fault tolerance:\n",
    "- Apache Hadoop continues to run even if one or several data processing nodes fail\n",
    "- It makes multiple copies of the same data block and stores them across several nodes\n",
    "- When a node fails, Hadoop retrieves the information from another node and prepares it for data processing.\n",
    "- Meanwhile, Apache Spark relies on a special data processing technology called Resilient Distributed Dataset (RDD).\n",
    "- With RDD, Apache Spark remembers how it retrieves specific information from storage and can reconstruct the data if the underlying storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c587694-b34d-4e43-b0a6-9175830abc83",
   "metadata": {},
   "source": [
    "Key Components :\n",
    "\n",
    "a. Hadoop Components :\n",
    "    Apache Hadoop has four main components:\n",
    "- Hadoop Distributed File System (HDFS) is a special file system that stores large datasets across multiple computers.\n",
    "- These computers are called Hadoop clusters.\n",
    "- Yet Another Resource Negotiator (YARN) schedules tasks and allocates resources to applications running on Hadoop.\n",
    "- Hadoop MapReduce allows programs to break large data processing tasks into smaller ones and runs them in parallel on multiple servers.\n",
    "- Hadoop Common, or Hadoop Core, provides the necessary software libraries for other Hadoop components.raphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af9711b-64cb-4328-a2aa-a7a0d3ddd452",
   "metadata": {},
   "source": [
    "b. Spark Components :\n",
    "    Apache Spark runs with the following components:\n",
    "- Spark Core coordinates the basic functions of Apache Spark.\n",
    "- These functions include memory management, data storage, task scheduling, and data processing.\n",
    "- Spark SQL allows you to process data in Spark's distributed storage.\n",
    "- Spark Streaming and Structured Streaming allow Spark to stream data efficiently in real time by separating data into tiny continuous blocks.\n",
    "- Machine Learning Library (MLlib) provides several machine learning algorithms that you can apply to big data.\n",
    "- GraphX allows you to visualize and analyze data with graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ef873a-c651-4cb3-aa07-1c7353c164fc",
   "metadata": {},
   "source": [
    "### Question 4. Can you write a query to find the employee count under each manager?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5556e595-dc20-440e-a2b5-3049696a4151",
   "metadata": {},
   "source": [
    "The Dataset: Let's start with the dataset. We have a list of employees with the following columns:\n",
    "\n",
    "employee_id: The unique identifier for each employee.\n",
    "first_name: The first name of the employee.\n",
    "last_name: The last name of the employee.\n",
    "manager_id: The ID of the employee's manager.\n",
    "We're selecting the manager_id and the count of employees (no_of_emp) managed by each manager.\n",
    "We use a self-join on the \"EMP\" view to associate employees with their respective managers.\n",
    "We group the results by manager_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "847c933a-91ea-4c13-b335-ca351786b9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('4529', 'Nancy', 'Young', '4125'),\n",
    "('4238','John', 'Simon', '4329'),\n",
    "('4329', 'Martina', 'Candreva', '4125'),\n",
    "('4009', 'Klaus', 'Koch', '4329'),\n",
    "('4125', 'Mafalda', 'Ranieri', 'NULL'),\n",
    "('4500', 'Jakub', 'Hrabal', '4529'),\n",
    "('4118', 'Moira', 'Areas', '4952'),\n",
    "('4012', 'Jon', 'Nilssen', '4952'),\n",
    "('4952', 'Sandra', 'Rajkovic', '4529'),\n",
    "('4444', 'Seamus', 'Quinn', '4329')]\n",
    "schema = ['employee_id' ,'first_name', 'last_name', 'manager_id']\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9705430e-8329-4bb2-8941-9bebb0e92ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"EMP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3242c1e2-5600-4f5f-90e2-597a6d2deccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------+\n",
      "|manager_id|no_of_emp|manager_name|\n",
      "+----------+---------+------------+\n",
      "|      4125|        2|     Mafalda|\n",
      "|      4329|        3|     Martina|\n",
      "|      4529|        2|       Nancy|\n",
      "|      4952|        2|      Sandra|\n",
      "+----------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            select e.manager_id as manager_id, \n",
    "            count(e.employee_id) as no_of_emp, m.first_name as manager_name from EMP e\n",
    "            inner join EMP m        \n",
    "            on m.employee_id = e.manager_id\n",
    "            group by e.manager_id, m.first_name\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0ca5b07-725d-40ab-8960-4d870dea5347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+----------+------------+\n",
      "|employee_id|first_name|last_name|manager_id|manager_name|\n",
      "+-----------+----------+---------+----------+------------+\n",
      "|       4529|     Nancy|    Young|      4125|     Mafalda|\n",
      "|       4329|   Martina| Candreva|      4125|     Mafalda|\n",
      "|       4238|      John|    Simon|      4329|     Martina|\n",
      "|       4009|     Klaus|     Koch|      4329|     Martina|\n",
      "|       4444|    Seamus|    Quinn|      4329|     Martina|\n",
      "|       4500|     Jakub|   Hrabal|      4529|       Nancy|\n",
      "|       4952|    Sandra| Rajkovic|      4529|       Nancy|\n",
      "|       4118|     Moira|    Areas|      4952|      Sandra|\n",
      "|       4012|       Jon|  Nilssen|      4952|      Sandra|\n",
      "+-----------+----------+---------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = df.alias(\"e\").join(df.alias(\"m\"), col(\"e.manager_id\") == col(\"m.employee_id\"), \"inner\") \\\n",
    "            .select(col(\"e.employee_id\"), col(\"e.first_name\"), col(\"e.last_name\"), col(\"e.manager_id\"), col(\"m.first_name\").alias(\"manager_name\"))\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a22f1b2-c5b3-49cc-a027-0cd950ed0745",
   "metadata": {},
   "source": [
    "### Question 5. Write a Pyspark code to find the output table as given below- employeeid, default_number, total_entry, total_login, total_logout, latest_login, latest_logout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8da49e7-277d-46d5-8bd1-a4274bf0fac3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1987548079.py, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[21], line 30\u001b[1;36m\u001b[0m\n\u001b[1;33m    '''the resultsthe results.\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- The first step is to create two DataFrames called checkin_df and detail_df. \n",
    "- The checkin_df DataFrame contains the following columns: \n",
    "    a. employeeid: The employee ID \n",
    "    b. entry_details: The type of entry (login or logout) \n",
    "    c. timestamp_details: The timestamp of the entry\r\n",
    "- T\n",
    "The detail_df DataFrame contains the following columns\n",
    "    a. : id: The employee I\n",
    "    b. D phone_number: The employee's phone numbe\n",
    "    c. r isdefault: A flag indicating whether the employee is a default user- \r\n",
    "\r\n",
    "The next step is to join the two DataFrames on the employeeid colu\n",
    "- mn. This will create a new DataFrame called joined_df that contains all of the data from both DataFrame- s.\r\n",
    "\r\n",
    "The next step is to filter the joined_df DataFrame to only include rows where the isdefault column is eq al to \n",
    "- true. This will ensure that we only consider default users in our analy- sis.\r\n",
    "\r\n",
    "The next step is to create three separate DataF    a. rames:\r\n",
    "\r\n",
    "total_entry_df: This df will contain the total number of entries for each \n",
    "    b. employee. total_login_df: This df will contain the total number of logins for each \n",
    "    c. employee. latest_login_df: This df will contain the latest login timestamp for each \n",
    "    \n",
    "- employee. To create these DataFrames, we use the groupBy() and agg() f\n",
    "- unctions. The groupBy() function groups the DataFrame by the employeeid column, and the agg() function calculates the total number of entries, total number of logins, and latest login timestamp for eac- h group.\r\n",
    "\r\n",
    "The final step is to join the three DF's together to create the final\n",
    "-  DataFrame. We use the join() function with the on and how arguments to join the DF's on the employe\n",
    "'''the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56f5b9b0-9d6d-45ae-8138-f7081077c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_df = spark.createDataFrame([(1000, 'login', '2023-06-16 01:00:15.34'),\n",
    "                                    (1000, 'login', '2023-06-16 02:00:15.34'),\n",
    "                                    (1000, 'login', '2023-06-16 03:00:15.34'),\n",
    "                                    (1000, 'logout', '2023-06-16 12:00:15.34'),\n",
    "                                    (1001, 'login', '2023-06-16 01:00:15.34'),\n",
    "                                    (1001, 'login', '2023-06-16 02:00:15.34'),\n",
    "                                    (1001, 'login', '2023-06-16 03:00:15.34'),\n",
    "                                    (1001, 'logout', '2023-06-16 12:00:15.34')],\n",
    "                                   [\"employeeid\", \"entry_details\", \"timestamp_details\"])\n",
    "\n",
    "detail_df = spark.createDataFrame([(1001, 9999, 'false'),\n",
    "                                   (1001, 1111, 'false'),\n",
    "                                   (1001, 2222, 'true'),\n",
    "                                   (1003, 3333, 'false')],\n",
    "                                  [\"id\", \"phone_number\", \"isdefault\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43afa535-afda-4ef6-8969-159f682ad1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+----+------------+---------+\n",
      "|employeeid|entry_details|   timestamp_details|  id|phone_number|isdefault|\n",
      "+----------+-------------+--------------------+----+------------+---------+\n",
      "|      1001|        login|2023-06-16 01:00:...|1001|        9999|    false|\n",
      "|      1001|        login|2023-06-16 01:00:...|1001|        1111|    false|\n",
      "|      1001|        login|2023-06-16 01:00:...|1001|        2222|     true|\n",
      "|      1001|        login|2023-06-16 02:00:...|1001|        9999|    false|\n",
      "|      1001|        login|2023-06-16 02:00:...|1001|        1111|    false|\n",
      "|      1001|        login|2023-06-16 02:00:...|1001|        2222|     true|\n",
      "|      1001|        login|2023-06-16 03:00:...|1001|        9999|    false|\n",
      "|      1001|        login|2023-06-16 03:00:...|1001|        1111|    false|\n",
      "|      1001|        login|2023-06-16 03:00:...|1001|        2222|     true|\n",
      "|      1001|       logout|2023-06-16 12:00:...|1001|        9999|    false|\n",
      "|      1001|       logout|2023-06-16 12:00:...|1001|        1111|    false|\n",
      "|      1001|       logout|2023-06-16 12:00:...|1001|        2222|     true|\n",
      "+----------+-------------+--------------------+----+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df = checkin_df.join(detail_df, checkin_df.employeeid == detail_df.id)\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad99b874-e46f-4fd7-9455-11106c8ad8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+----+------------+---------+\n",
      "|employeeid|entry_details|   timestamp_details|  id|phone_number|isdefault|\n",
      "+----------+-------------+--------------------+----+------------+---------+\n",
      "|      1001|        login|2023-06-16 01:00:...|1001|        2222|     true|\n",
      "|      1001|        login|2023-06-16 02:00:...|1001|        2222|     true|\n",
      "|      1001|        login|2023-06-16 03:00:...|1001|        2222|     true|\n",
      "|      1001|       logout|2023-06-16 12:00:...|1001|        2222|     true|\n",
      "+----------+-------------+--------------------+----+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df = joined_df.where(joined_df[\"isdefault\"] == 'true')\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aecf63d1-3189-4f10-802c-618054d57095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|employeeid|Total_entry|\n",
      "+----------+-----------+\n",
      "|      1001|          4|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "total_entry_df = joined_df.groupBy('employeeid').agg(count('*')\n",
    "                                                     .alias('Total_entry'))\n",
    "total_entry_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73dabe13-0a9c-4686-8ca1-9c267bb65c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|employeeid|total_login|\n",
      "+----------+-----------+\n",
      "|      1001|          3|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_login_df = joined_df.filter(joined_df['entry_details'] == 'login').groupBy('employeeid').agg(count('*').alias('total_login'))\n",
    "total_login_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10ce7321-a35e-4bb2-a7f4-b1270d38c267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+\n",
      "|employeeid|latest_login          |\n",
      "+----------+----------------------+\n",
      "|1001      |2023-06-16 03:00:15.34|\n",
      "+----------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first\n",
    "latest_login_df = joined_df.filter(joined_df['entry_details'] == 'login').orderBy(joined_df['timestamp_details'].desc()).groupBy('employeeid').agg(first('timestamp_details').alias('latest_login'))\n",
    "latest_login_df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76fe2197-d30c-491b-ab67-fc055153fe77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|employeeid|       latest_logout|\n",
      "+----------+--------------------+\n",
      "|      1001|2023-06-16 12:00:...|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latest_logout_df = joined_df.filter(joined_df['entry_details'] == 'logout').\\\n",
    "            orderBy(joined_df['timestamp_details'].desc()).groupBy('employeeid').\\\n",
    "            agg(first('timestamp_details').alias('latest_logout'))\n",
    "latest_logout_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00021964-bb74-4aa5-a9e4-10a3bdd17f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = total_entry_df.join(total_login_df, on='employeeid', how='inner')\n",
    "final_df = final_df.join(latest_login_df, on='employeeid', how='inner')\n",
    "final_df = final_df.join(latest_logout_df, on='employeeid', how='inner')\n",
    "\n",
    "final_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
